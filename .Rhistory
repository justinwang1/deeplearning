labels.hat <- rep(-1,num)
for(i in 1:num) {
res <- feed(test.data[i,],W.s)
labels.hat[i] <- ifelse(res[1] > res[2],1,0)
}
test.error <- sum(abs(labels.hat-test.labels))/num; test.error
##Create the data. We are taking a subset of the 0's and 1's only in this script to perform binary classification.
#1st par: Train sample size, 2nd par: test sample size, 3rd par: which digits to use?
train.size <- 5000; test.size <- 1000; digits <- c(0,1)
mysamp <- sample.mnist(train.size,test.size,digitstosample=digits)
train.data <- mysamp$train.data; train.labels <- mysamp$train.labels
test.data <- mysamp$test.data; test.labels <- mysamp$test.labels
##Pretrain the weights using Deep Belief Network. Adjust the number of hidden layers and nodes in each hidden layer as
#needed. For example, the default, hiddenCounts <- c(250,50,25), creates 3 hidden layers, with 250, 50, and 25 nodes
#respectively.
#Note: If training deep autoencoder (unsupervised learning), make sure that the hidden layer counts are symmetric.
#Ex. hiddenCounts <- c(250,50,25,50,250)
hiddenCounts <- c(5); epochs <- 20 #Set the parameters for training Deep Belief Network
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
##Source other files to set up the model
source("rbm.R") #Function to train a Restricted Boltzmann Machine
source("dbn.R") #Function to train a Deep Belief Network
hiddenCounts <- c(5); epochs <- 20 #Set the parameters for training Deep Belief Network
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
rate <- 0.05; iters <- 3
mode <- "classification"
layerCounts <- c(hiddenCounts,ifelse(mode=="classification",2,dim(train.data[2])))
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(train.data,train.labels,layerCounts,W.start,rate,iters,mode=mode,verbose=TRUE)
##Get test error for MNIST digit dataset 1's and 0's
num <- dim(test.data)[1]
labels.hat <- rep(-1,num)
for(i in 1:num) {
res <- feed(test.data[i,],W.s)
labels.hat[i] <- ifelse(res[1] > res[2],1,0)
}
test.error <- sum(abs(labels.hat-test.labels))/num; test.error
##Source all the prequisite scripts in order to make the analysis work
#Util.R is a script of helper functions useful in most of the other scripts in the project
#RealityMiningSetup.R cleans the data (originally in Matlab matrix format) for usage
#Main.R runs the deep neural network model used in the analysis
source("Util.R")
#Make sure rmadjacency.mat is in the same folder as RealityMiningSetup.R. Also need to install package "R.matlab".
source("RealityMiningSetup.R")
#Trains a neural network using backpropagation. Works for any number of hidden layers. Written for binary classification.
#Note: the paramter mode can be set to either "classification" or "autoencoder". If the latter, it trains a deep autoencoder,
#a nonsupervised variant of deep neural networks whose output is the input itself. In other words, an autoencoder "reconstructs"
#the input. It is useful for its middle code layer which can be used for nonlinear dimension reduction.
train.nn <- function(data,layerCounts,W,rate=0.05,training.iters=20,
mode="classification",labels=NULL,verbose=FALSE)
{
#Helper Functions
sigm <- function(y) {
return(1/(1+exp(-y)))
}
output.err <- function(target,output) {
output*(1-output)*(target-output)
}
hidden.err <- function(outputs,prev.error,layernum) {
#Input=0, Hidden 1=1, ..., Output=(numhiddenout)
stopifnot(layernum >= 1 && layernum <= length(layerCounts))
H <- outputs[[layernum]]
numinlayer <- layerCounts[layernum]
weights <- W[[layernum+1]]
prod <- (apply(weights,1,function(inp)
{ return(sum(inp*prev.error)) }))[-1]
err <- H*(1-H)*prod; err
}
#Main
n <- dim(data)[1]; A <- training.iters
for(a in 1:A)
{
#If verbose (default), print progress each iteration
if(verbose)	{
cat("Iteration ",a,"\n")
}
for(i in 1:n)
{
#Feedforward
numhiddenout <- length(layerCounts)
input <- data[i,]
if(mode == "autoencoder") {
target <- input
}
else if(mode == "classification") {
target <- c(0.9,0.1)
if(!labels[i])  target <- rev(target)
}
else {
stop("Invalid Mode.")
}
outputs <- feed(input,W,"all"); output <- outputs[[numhiddenout]]
#Backpass
errors <- list()
errors[[1]] <- output.err(target,output)
for(k in 1:(numhiddenout-1)) {
errors[[k+1]] <- hidden.err(outputs,errors[[k]],numhiddenout-k)
}
errors <- rev(errors)
#Updates hidden weights
in.values <- list(); in.values[[1]] <- input
for(h in 1:(numhiddenout-1)) {
in.values[[h+1]] <- outputs[[h]]
}
for(s in numhiddenout:1) {
valsPrev <- c(1,in.values[[s]])
errorsCurr <- errors[[s]]
lenCurr <- length(errorsCurr)
W.delta <- sweep(replicate(lenCurr,valsPrev),MARGIN=2,errorsCurr,`*`)
W[[s]] <- W[[s]] + rate*W.delta
}
}
}
return(W)
}
W.s <- train.nn(train.data,layerCounts,W.start,rate,iters,mode=mode,train.labels,verbose=TRUE)
batchdata <- makebatches(data)
hiddenCounts <- c(5)
W.init <- train.dbn(hiddenCounts,batchdata,epochs=20)
rate <- 0.05; training.iters <- 3
p <- dim(data)[2]
layerCounts <- c(hiddenCounts,p)
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(data,layerCounts,W.start,rate,training.iters,mode="autoencoder",verbose=TRUE)
hiddenCounts <- c(5)
W.init <- train.dbn(hiddenCounts,data,epochs=20)
rate <- 0.05; training.iters <- 3
mode <- "autoencoder"
layerCounts <- c(hiddenCounts,ifelse(mode=="classification",2,dim(train.data[2])))
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(data,layerCounts,W.start,rate,training.iters,mode=mode,verbose=TRUE)
##Extract code layer
code.length <- hiddenCounts[length(hiddenCounts)]
codes <- generate.codes(data,W.init,code.length)
##Cluster using K Means with K = 2.
#D refers to clustering results using original data.
#C refers to clustering results after dimension reduction, via extracting the code layer from a deep autoencoder.
D <- kmeans(data,2)$cluster-1; C <- kmeans(codes,2)$cluster-1
sparsity <- apply(data,1,mean); hist(sparsity)
C.ones <- sparsity[which(C==1)]; C.zeros <- sparsity[which(C==0)]
D.ones<- sparsity[which(D==1)]; D.zeros <- sparsity[which(D==0)]
boxplot(C.ones,C.zeros,D.ones,D.zeros,names=c("DL 1","DL 2","KM 1","KM 2"),
main="Clusters when K=2")
hist(sparsity)
abline(v=min(C.ones),col=4); abline(v=max(C.zeros),col=4,lty=2)
abline(v=min(D.ones),col=2); abline(v=max(D.zeros),col=2,lty=2)
D3 <- kmeans(data,3)$cluster-1; C3 <- kmeans(codes,3)$cluster-1
D3 <- clustering.adjust(D3,C3)
D.0 <- data[which(D3==0),]; D.1 <- data[which(D3==1),]; D.2 <- data[which(D3==2),]
C.0 <- data[which(C3==0),]; C.1 <- data[which(C3==1),]; C.2 <- data[which(C3==2),]
#Setup
D3 <- kmeans(data,3)$cluster-1; C3 <- kmeans(codes,3)$cluster-1
D.0 <- data[which(D3==0),]; D.1 <- data[which(D3==1),]; D.2 <- data[which(D3==2),]
C.0 <- data[which(C3==0),]; C.1 <- data[which(C3==1),]; C.2 <- data[which(C3==2),]
#Look at sample images of the three different clusters
where <- 4:6
par(mfrow=c(3,3))
for(i in where) image(matrix(D.0[i,],92,92))
for(i in where) image(matrix(D.1[i,],92,92))
for(i in where) image(matrix(D.2[i,],92,92))
par(mfrow=c(1,1))
D.zeros.3 <- sparsity[which(D3==0)]; D.ones.3 <- sparsity[which(D3==1)]
D.twos.3 <- sparsity[which(D3==2)]; C.zeros.3 <- sparsity[which(C3==0)]
C.ones.3 <- sparsity[which(C3==1)]; C.twos.3 <- sparsity[which(C3==2)]
boxplot(C.ones.3,C.zeros.3,C.twos.3,D.ones.3,D.zeros.3,D.twos.3,
names=c("C1","C2","C3","D1","D2","D3"),main="Clusters when K=3")
where <- 4:6
par(mfrow=c(3,3))
for(i in where) image(matrix(D.0[i,],92,92))
for(i in where) image(matrix(D.1[i,],92,92))
for(i in where) image(matrix(D.2[i,],92,92))
##Source other files to set up the model
source("rbm.R") #Function to train a Restricted Boltzmann Machine
source("dbn.R") #Function to train a Deep Belief Network
source("Util.R") #Utility function consisting of helper functions
source("nn.R") #Function to train a Neural Network w/ any number of hidden layers
source("MNIST.R") #Sets up the MNIST data. Make sure the MNIST data files are in the same folder as the
#file MNIST.R.
##Create the data. We are taking a subset of the 0's and 1's only in this script to perform binary classification.
#1st par: Train sample size, 2nd par: test sample size, 3rd par: which digits to use?
train.size <- 5000; test.size <- 1000; digits <- c(0,1)
mysamp <- sample.mnist(train.size,test.size,digitstosample=digits)
train.data <- mysamp$train.data; train.labels <- mysamp$train.labels
test.data <- mysamp$test.data; test.labels <- mysamp$test.labels
##Pretrain the weights using Deep Belief Network. Adjust the number of hidden layers and nodes in each hidden layer as
#needed. For example, the default, hiddenCounts <- c(250,50,25), creates 3 hidden layers, with 250, 50, and 25 nodes
#respectively.
#Note: If training deep autoencoder (unsupervised learning), make sure that the hidden layer counts are symmetric.
#Ex. hiddenCounts <- c(250,50,25,50,250)
hiddenCounts <- c(5); epochs <- 20 #Set the parameters for training Deep Belief Network
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
rate <- 0.05; iters <- 3
mode <- "classification"
layerCounts <- c(hiddenCounts,ifelse(mode=="classification",2,dim(train.data[2])))
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(train.data,layerCounts,W.start,rate,iters,mode=mode,train.labels,verbose=TRUE)
rbm <- function(batchdata,numhid,maxepoch=10)
{
#Set parameters
restart <- 1; weightcost <- 0.0002
epsilonw <- 0.1; epsilonvb <- 0.1; epsilonhb <- 0.1
initialmomentum <- 0.5; finalmomentum <- 0.9
numcases <- dim(batchdata)[1]; numdims <- dim(batchdata)[2]; numbatches <- dim(batchdata)[3]
if(restart == 1) {
restart <- 0
epoch <- 1
}
#Initialize the variables
vishid <- 0.1*randn(numdims,numhid); hidbiases <- zeros(1,numhid); visbiases <- zeros(1,numdims)
poshidprobs <- zeros(numcases,numhid); neghidprobs <- zeros(numcases,numhid)
vishidinc <- zeros(numdims,numhid); hidbiasinc <- zeros(1,numhid); visbiasinc <- zeros(1,numdims)
batchposhidprobs <- zeros(numcases,numhid,numbatches)
#Loop over epochs
for(epoch in epoch:maxepoch) {
errsum <- 0
for(batch in 1:numbatches) {
#Positive phase
data <- batchdata[ , ,batch]
poshidprobs <- 1/(1 + exp(-data %*% vishid - repmat(hidbiases,numcases)))
batchposhidprobs[ , ,batch] <- poshidprobs
posprods <- t(data) %*% poshidprobs
poshidact <- sumM(poshidprobs); posvisact <- sumM(data)
poshidstates <- (poshidprobs > rand(numcases,numhid))*1
#Negative phase
negdata <- 1/(1 + exp(-poshidstates %*% t(vishid) - repmat(visbiases,numcases)))
neghidprobs <- 1/(1 + exp(-negdata %*% vishid - repmat(hidbiases,numcases)))
negprods <- t(negdata) %*% neghidprobs
neghidact <- sumM(neghidprobs); negvisact <- sumM(negdata)
err <- sum((data-negdata)^2)
errsum <- err + errsum
if(epoch > 5)
momentum <- finalmomentum
else
momentum <- initialmomentum
#Update weights and biases
vishidinc <- momentum*vishidinc +
epsilonw*( (posprods - negprods)/numcases - weightcost*vishid)
visbiasinc <- momentum*visbiasinc + (epsilonvb/numcases)*(posvisact - negvisact)
hidbiasinc <- momentum*hidbiasinc + (epsilonhb/numcases)*(poshidact - neghidact)
vishid <- vishid + vishidinc
visbiases <- visbiases + visbiasinc
hidbiases <- hidbiases + hidbiasinc
}
cat("Epoch ",epoch,", Epoch Error: "," ",errsum,"\n")
}
list(vishid,hidbiases,visbiases,batchposhidprobs)
}
zeros <- function(a,b,c=NA) {
if(is.na(c)) {
array(rep(0,a*b),c(a,b))
} else {
array(rep(0,a*b*c),c(a,b,c))
}
}
randn <- function(a,b){
matrix(rnorm(a*b),a,b)
}
rand <- function(a,b) {
matrix(runif(a*b),a,b)
}
repmat <- function(A,n) {
stopifnot(dim(A)[1] == 1)
At <- as.vector(t(A))
return(t(replicate(n,At)))
}
sumM <- function(A) {
t(apply(A,2,sum))
}
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
rbm <- function(batchdata,numhid,maxepoch=10)
{
#Set parameters
restart <- 1; weightcost <- 0.0002
epsilonw <- 0.1; epsilonvb <- 0.1; epsilonhb <- 0.1
initialmomentum <- 0.5; finalmomentum <- 0.9
numcases <- dim(batchdata)[1]; numdims <- dim(batchdata)[2]; numbatches <- dim(batchdata)[3]
if(restart == 1) {
restart <- 0
epoch <- 1
}
#Initialize the variables
vishid <- 0.1*randn(numdims,numhid); hidbiases <- zeros(1,numhid); visbiases <- zeros(1,numdims)
poshidprobs <- zeros(numcases,numhid); neghidprobs <- zeros(numcases,numhid)
vishidinc <- zeros(numdims,numhid); hidbiasinc <- zeros(1,numhid); visbiasinc <- zeros(1,numdims)
batchposhidprobs <- zeros(numcases,numhid,numbatches)
#Loop over epochs
for(epoch in epoch:maxepoch) {
errsum <- 0
for(batch in 1:numbatches) {
#Positive phase
data <- batchdata[ , ,batch]
poshidprobs <- 1/(1 + exp(-data %*% vishid - repmat(hidbiases,numcases)))
batchposhidprobs[ , ,batch] <- poshidprobs
posprods <- t(data) %*% poshidprobs
poshidact <- sumM(poshidprobs); posvisact <- sumM(data)
poshidstates <- (poshidprobs > rand(numcases,numhid))*1
#Negative phase
negdata <- 1/(1 + exp(-poshidstates %*% t(vishid) - repmat(visbiases,numcases)))
neghidprobs <- 1/(1 + exp(-negdata %*% vishid - repmat(hidbiases,numcases)))
negprods <- t(negdata) %*% neghidprobs
neghidact <- sumM(neghidprobs); negvisact <- sumM(negdata)
err <- sum((data-negdata)^2)
errsum <- err + errsum
if(epoch > 5)
momentum <- finalmomentum
else
momentum <- initialmomentum
#Update weights and biases
vishidinc <- momentum*vishidinc +
epsilonw*( (posprods - negprods)/numcases - weightcost*vishid)
visbiasinc <- momentum*visbiasinc + (epsilonvb/numcases)*(posvisact - negvisact)
hidbiasinc <- momentum*hidbiasinc + (epsilonhb/numcases)*(poshidact - neghidact)
vishid <- vishid + vishidinc
visbiases <- visbiases + visbiasinc
hidbiases <- hidbiases + hidbiasinc
}
cat("Epoch ",epoch,"    Epoch Error: "," ",errsum,"\n")
}
list(vishid,hidbiases,visbiases,batchposhidprobs)
}
zeros <- function(a,b,c=NA) {
if(is.na(c)) {
array(rep(0,a*b),c(a,b))
} else {
array(rep(0,a*b*c),c(a,b,c))
}
}
randn <- function(a,b){
matrix(rnorm(a*b),a,b)
}
rand <- function(a,b) {
matrix(runif(a*b),a,b)
}
repmat <- function(A,n) {
stopifnot(dim(A)[1] == 1)
At <- as.vector(t(A))
return(t(replicate(n,At)))
}
sumM <- function(A) {
t(apply(A,2,sum))
}
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
##Source other files to set up the model
source("rbm.R") #Function to train a Restricted Boltzmann Machine
source("dbn.R") #Function to train a Deep Belief Network
source("Util.R") #Utility function consisting of helper functions
source("nn.R") #Function to train a Neural Network w/ any number of hidden layers
source("MNIST.R") #Sets up the MNIST data. Make sure the MNIST data files are in the same folder as the
#file MNIST.R.
##Create the data. We are taking a subset of the 0's and 1's only in this script to perform binary classification.
#1st par: Train sample size, 2nd par: test sample size, 3rd par: which digits to use?
train.size <- 5000; test.size <- 1000; digits <- c(0,1)
mysamp <- sample.mnist(train.size,test.size,digitstosample=digits)
train.data <- mysamp$train.data; train.labels <- mysamp$train.labels
test.data <- mysamp$test.data; test.labels <- mysamp$test.labels
##Pretrain the weights using Deep Belief Network. Adjust the number of hidden layers and nodes in each hidden layer as
#needed. For example, the default, hiddenCounts <- c(250,50,25), creates 3 hidden layers, with 250, 50, and 25 nodes
#respectively.
#Note: If training deep autoencoder (unsupervised learning), make sure that the hidden layer counts are symmetric.
#Ex. hiddenCounts <- c(250,50,25,50,250)
hiddenCounts <- c(200,50,25); epochs <- 20 #Set the parameters for training Deep Belief Network
W.init <- train.dbn(hiddenCounts,train.data,epochs=epochs)
##Train a deep neural network using the pretrained weights from above as the initial weights. In practice, the initial
#weights are usually "most of the way there" already, making this step more akin to fine-tuning. If the weights were
#randomly initialized rather than pretrained, then the training algorithm (backpropagation) would "get lost" and be
#unable to find the local minimum needed.
rate <- 0.05; iters <- 10
#the two possible modes are "classification" and "autoencoder". The latter is used in RealityMiningAnalysis.R.
mode <- "classification"
layerCounts <- c(hiddenCounts,ifelse(mode=="classification",2,dim(train.data[2])))
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(train.data,layerCounts,W.start,rate,iters,mode=mode,train.labels,verbose=TRUE)
#Training takes about 5 minutes with default parameters
rate <- 0.05; iters <- 5
#The two possible modes are "classification" and "autoencoder". The latter is used in RealityMiningAnalysis.R.
mode <- "classification"
outlayernum <- ifelse(mode=="classification",2,dim(train.data[2])); layerCounts <- c(hiddenCounts,outlayernum)
W.start <- append.dbn.init(W.init,layerCounts)
W.s <- train.nn(train.data,layerCounts,W.start,rate,iters,mode=mode,train.labels,verbose=TRUE)
##Get test error for MNIST digit dataset 1's and 0's
num <- dim(test.data)[1]
labels.hat <- rep(-1,num)
for(i in 1:num) {
res <- feed(test.data[i,],W.s)
labels.hat[i] <- ifelse(res[1] > res[2],1,0)
}
test.error <- sum(abs(labels.hat-test.labels))/num; test.error
##Set seed so results can be compared for various parameter tweaks
set.seed(123456)
train.dbn <- function(hiddenCounts,data,epochs=10) {
#Convert the data to batch data
batchdata <- makebatches(data)
#How many epochs to train each?
numtrain <- length(hiddenCounts)
myepochs <- rep(0,numtrain)
if(length(epochs) > 1) {
myepochs <- epochs
} else  {
myepochs <- rep(epochs,numtrain)
}
#Run the first RBM
resultsList <- list()
results <- rbm(batchdata,numhid=hiddenCounts[1],maxepoch=myepochs[1])
resultsList[[1]] <- results
hidden <- results[[4]]
#Run the additional RBMs
if(!is.na(hiddenCounts[2]))
{
for(i in 2:numtrain)
{
results <- rbm(hidden,numhid=hiddenCounts[i],maxepoch=myepochs[i])
resultsList[[i]] <- results
hidden <- results[[4]]
}
}
#Bind the results into a list
myResults <- bind(resultsList); myResults
}
bind <- function(results) {
W.init <- list()
num <- length(results)
#Into code layer
for(i in 1:num)
{
curr.res <- results[[i]]
W.init[[i]] <- rbind(curr.res[[2]],curr.res[[1]])
}
#out of code layer
k <- num + 1
for(j in num:1)
{
curr.res <- results[[j]]
W.init[[k]] <- rbind(curr.res[[3]],t(curr.res[[1]]))
k <- k+1
}
W.init
}
makebatches <- function(data) {
totnum <- dim(data)[1]
randomorder <- 1:totnum
numbatches <- totnum/100
numdims <- dim(data)[2]
batchsize <- 100
batchdata <- zeros(batchsize, numdims, numbatches)
for(b in 1:numbatches)
{
batchdata[ , ,b] <- data[randomorder[(1+(b-1)*batchsize):(b*batchsize)], ]
}
batchdata
}
#Trains a Deep Belief Network model in order to pretrain the initial weights for a deep neural network. In practice,
#the resulting initial weights trained by DBN are usually in the ballpark already of the local minimum.
train.dbn <- function(hiddenCounts,data,epochs=10) {
#Convert the data to batch data
batchdata <- makebatches(data)
#How many epochs to train each?
numtrain <- length(hiddenCounts)
myepochs <- rep(0,numtrain)
if(length(epochs) > 1) {
myepochs <- epochs
} else  {
myepochs <- rep(epochs,numtrain)
}
#Run the first RBM
resultsList <- list()
results <- rbm(batchdata,numhid=hiddenCounts[1],maxepoch=myepochs[1])
resultsList[[1]] <- results
hidden <- results[[4]]
#Run the additional RBMs
if(!is.na(hiddenCounts[2]))
{
for(i in 2:numtrain)
{
results <- rbm(hidden,numhid=hiddenCounts[i],maxepoch=myepochs[i])
resultsList[[i]] <- results
hidden <- results[[4]]
}
}
#Bind the results into a list
myResults <- bind(resultsList); myResults
}
bind <- function(results) {
W.init <- list()
num <- length(results)
#Into code layer
for(i in 1:num)
{
curr.res <- results[[i]]
W.init[[i]] <- rbind(curr.res[[2]],curr.res[[1]])
}
#out of code layer
k <- num + 1
for(j in num:1)
{
curr.res <- results[[j]]
W.init[[k]] <- rbind(curr.res[[3]],t(curr.res[[1]]))
k <- k+1
}
W.init
}
makebatches <- function(data) {
totnum <- dim(data)[1]
randomorder <- 1:totnum
numbatches <- totnum/100
numdims <- dim(data)[2]
batchsize <- 100
batchdata <- zeros(batchsize, numdims, numbatches)
for(b in 1:numbatches) {
batchdata[ , ,b] <- data[randomorder[(1+(b-1)*batchsize):(b*batchsize)], ]
}
batchdata
}
